{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AnI8AtJtNnZr"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator=pipeline('text-generation',model='gpt2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D3SG4oANy82",
        "outputId": "d4d0f035-975f-4875-d2c4-184c6460cf78"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=[\"today is the beautiful day\",\n",
        "        \"In 2030, education will look like\",\n",
        "    \"The future of healthcare with AI is\"]\n",
        "\n",
        "outputs=[]\n",
        "\n",
        "for p in prompt:\n",
        "  results=generator(p,max_length=40,num_return_sequences=1)\n",
        "  text_output=f\"prompt: {p}\\n generated:{results[0]['generated_text']}\\n\"\n",
        "  outputs.append(text_output)\n",
        "  print(text_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5QcSYc0N9St",
        "outputId": "df2e720a-5657-4a5b-d674-739a00728dc6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: today is the beautiful day\n",
            " generated:today is the beautiful day of the last day of the year, and it is the day that the sun will rise and shine upon the world, and the world will be made manifest.\n",
            "\n",
            "The day is called the great feast, and it is a day that a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and a man will perform his duty, and\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: In 2030, education will look like\n",
            " generated:In 2030, education will look like this:\n",
            "\n",
            "The world's most populous country will grow by 8 percent, according to the World Bank.\n",
            "\n",
            "A study released in May found that around 8.2 million people in the world live in poverty.\n",
            "\n",
            "And the world's largest economies will have the highest unemployment rate.\n",
            "\n",
            "As for the United States, it's looking at 4.5 percent from a recent report from the OECD.\n",
            "\n",
            "But that's in the midst of more than a decade of economic growth and a sluggish economy.\n",
            "\n",
            "And one of the biggest obstacles to the United States' economic growth is a combination of regulations, the economy's slow recovery, and the lack of investment.\n",
            "\n",
            "That's a problem that's been building since the early 1990s.\n",
            "\n",
            "The world's top five economies are all relatively young, with the United States accounting for about half of that growth.\n",
            "\n",
            "\"We're living in a time of unprecedented growth in education, which is something which has been sorely lacking in the past,\" said Robert Hall of the World Bank. \"And we now have the opportunity to invest in education and infrastructure that will benefit the entire world.\"\n",
            "\n",
            "And while it's impossible to put a price tag on what you can afford today, Hall said that even\n",
            "\n",
            "prompt: The future of healthcare with AI is\n",
            " generated:The future of healthcare with AI is looking bright and bright for healthcare. The current technology of healthcare is becoming increasingly more and more complex. The question is, how, and by whom?\n",
            "\n",
            "The future of healthcare with AI is looking bright and bright for healthcare. The current technology of healthcare is becoming increasingly more and more complex. The question is, how, and by whom?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"with_ai_outputs_day1\",\"w\",encoding=\"utf-8\") as f:\n",
        "  for outs in outputs:\n",
        "    f.write(outs+ \"\\n\")\n",
        "print(\"output_saved_day1_projects.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WArc76PlPZ6q",
        "outputId": "55720379-b054-44ff-d154-b5c81e99a4b5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output_saved_day1_projects.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"with_ai_outputs_day1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mPHGtS86QmaW",
        "outputId": "0bbf4f72-3248-4d14-b352-168f6588fb46"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ded92bd1-d33e-4ca7-bc21-59f8d2a4a927\", \"with_ai_outputs_day1\", 2823)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EQT8cDcFQ18z"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}